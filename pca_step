import pandas as pd
import numpy as np
from scipy.signal import welch
from scipy.stats import skew, kurtosis
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


# Function to calculate spectral features
def calculate_spectral_features(data, fs, nperseg):
    frequencies, psd = welch(data, fs=fs, nperseg=nperseg)
    spectral_centroid = np.sum(frequencies * psd) / np.sum(psd)
    spectral_bandwidth = np.sqrt(np.sum(((frequencies - spectral_centroid) ** 2) * psd) / np.sum(psd))
    spectral_flatness = np.exp(np.mean(np.log(psd))) / np.mean(psd)
    cumulative_energy = np.cumsum(psd)
    total_energy = cumulative_energy[-1]
    roll_off = frequencies[np.where(cumulative_energy >= 0.75 * total_energy)[0][0]]
    peak_psd_index = np.argmax(psd)
    peak_psd_frequency = frequencies[peak_psd_index]
    peak_psd_energy = psd[peak_psd_index]

    spectral_entropy = -np.sum((psd / np.sum(psd)) * np.log(psd / np.sum(psd) + np.finfo(float).eps))
    spectral_kurtosis = kurtosis(psd)
    spectral_slope = np.polyfit(frequencies, psd, 1)[0]

    return (
        spectral_centroid, spectral_bandwidth, spectral_flatness, roll_off,
        peak_psd_frequency, peak_psd_energy, spectral_entropy, spectral_kurtosis,
        spectral_slope
    )


# Function to calculate temporal features
def calculate_temporal_features(data):
    crest_factor = np.max(np.abs(data)) / np.sqrt(np.mean(data ** 2))
    energy = np.sum(data ** 2)
    skewness_value = skew(data)
    kurtosis_value = kurtosis(data)
    zero_crossing_rate = ((data[:-1] * data[1:]) < 0).sum() / len(data)
    return crest_factor, energy, skewness_value, kurtosis_value, zero_crossing_rate


# Function to perform PCA
def apply_pca(features, n_components=1):
    pca = PCA(n_components=n_components)
    pca_result = pca.fit_transform(features)
    explained_variance = pca.explained_variance_ratio_
    components = pca.components_
    return pca_result, explained_variance, components


# Main function with user-defined time intervals and fixed window size
def main_with_fixed_windows():
    # Load the CSV data
    file_path = 'veri/AFYON DEPREMİ/G-combined_afyon.csv'  # Dosya yolu buradan belirlenir.
    data = pd.read_csv(file_path)
    fs = 100  # Sampling frequency
    time = data.index / fs

    # Use the first column as the acceleration data
    acceleration_data = data.iloc[:, 0]

    # Plot acceleration vs time graph
    plt.figure(figsize=(10, 6))
    plt.plot(time, acceleration_data)
    plt.xlabel('Time (seconds)')
    plt.ylabel('Acceleration (g)')
    plt.title('Acceleration Time Series')
    plt.grid(True)
    plt.show()

    # Parameters for Welch's method and window size
    nperseg = 128  # Welch's method parameter

    # Get start time, end time, and fixed window size from the user
    try:
        start_time = float(input("Başlangıç zamanını girin (saniye): "))
        end_time = float(input("Bitiş zamanını girin (saniye): "))
        window_size = float(input("Sabit pencere boyutunu girin (saniye): "))
    except ValueError:
        print("Geçersiz giriş! Lütfen geçerli bir sayı girin.")
        return

    # List to store explained variance ratios
    pca_explained_variances = []
    window_times = []

    # Loop through the time intervals with the fixed window size
    window_results = []
    for window_start in np.arange(start_time, end_time, window_size):
        start_sample = int(window_start * fs)
        end_sample = int(min((window_start + window_size) * fs, len(acceleration_data)))

        if end_sample > len(acceleration_data):
            break

        selected_data_window = acceleration_data[start_sample:end_sample]

        # Calculate spectral and temporal features
        spectral_features = calculate_spectral_features(selected_data_window, fs, nperseg=nperseg)
        temporal_features = calculate_temporal_features(selected_data_window)

        result = {
            "Başlangıç Zamanı (saniye)": window_start,
            "Bitiş Zamanı (saniye)": end_sample / fs,
            "Spectral Centroid (Hz)": spectral_features[0],
            "Spectral Bandwidth (Hz)": spectral_features[1],
            "Spectral Flatness": spectral_features[2],
            "Spectral Roll-off Frequency (Hz)": spectral_features[3],
            "Peak PSD Frequency (Hz)": spectral_features[4],
            "Peak PSD Energy": spectral_features[5],
            "Spectral Entropy": spectral_features[6],
            "Spectral Kurtosis": spectral_features[7],
            "Spectral Slope": spectral_features[8],
            "Crest Factor": temporal_features[0],
            "Energy": temporal_features[1],
            "Skewness": temporal_features[2],
            "Kurtosis": temporal_features[3],
            "Zero-Crossing Rate": temporal_features[4]
        }
        window_results.append(result)

        # Convert results to DataFrame
        results_df = pd.DataFrame(window_results)

        # Extract features for PCA
        feature_columns = results_df.columns[2:]  # Only numerical features
        features = results_df[feature_columns]

        # Standardize the features
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)

        # Apply PCA for each window
        pca_result, explained_variance, components = apply_pca(features_scaled, n_components=1)

        # Store the explained variance ratio for plotting
        if not np.isnan(explained_variance[0]) and explained_variance[0] != 1.0:
            pca_explained_variances.append(explained_variance[0])
            window_times.append(window_start)

        # Display results for PCA1 and top 5 contributing features for each window
        component_df = pd.DataFrame(components, columns=feature_columns, index=['PC1'])
        print(f"\nPCA Component 1 explained variance ratio for window starting at {window_start} seconds: {explained_variance[0]}")

        sorted_features = component_df.iloc[0].abs().sort_values(ascending=False)
        top_features = sorted_features.index[:10]  # Show top 10 features
        print(f"\nTop 10 features contributing to PCA Component 1 for window starting at {window_start} seconds:")
        for i, feature in enumerate(top_features):
            print(f"{i+1}. {feature} (Loading: {component_df.iloc[0][feature]})")

    # Plot PCA explained variance ratio over time (excluding NaN and 1 values)
    plt.figure(figsize=(10, 6))
    plt.plot(window_times, pca_explained_variances, marker='o', linestyle='-', color='b')
    plt.xlabel('Window Start Time (seconds)')
    plt.ylabel('PCA Component 1 Explained Variance Ratio')
    plt.title('PCA Explained Variance Ratio Over Time (Excluding NaN and 1)')
    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    main_with_fixed_windows()
