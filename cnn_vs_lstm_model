import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import mean_squared_error
from scipy.signal import welch
from scipy.stats import skew, kurtosis
import pywt

from keras import Sequential, Input
from keras.layers import LSTM, Dense, Dropout, Bidirectional, Conv1D, GlobalAveragePooling1D
from keras.optimizers import Adamax

# LOF Hesaplama Fonksiyonu
def calculate_lof(features, n_neighbors=20):
    lof = LocalOutlierFactor(n_neighbors=n_neighbors, metric='cosine')
    lof_labels = lof.fit_predict(features)
    lof_scores = -lof.negative_outlier_factor_
    return lof_scores

# Wavelet Entropy hesaplama fonksiyonu
def calculate_wavelet_entropy(coeffs):
    total_energy = sum(np.sum(c ** 2) for c in coeffs)
    entropy = -sum(np.sum(c ** 2) / total_energy * np.log(np.sum(c ** 2) / total_energy) for c in coeffs)
    return entropy

# Spektral ve Zaman-Domain Özelliklerini Hesaplama Fonksiyonu
def calculate_features(data, fs):
    frequencies, psd = welch(data, fs=fs, nperseg=128)
    spectral_centroid = np.sum(frequencies * psd) / np.sum(psd)
    spectral_bandwidth = np.sqrt(np.sum(((frequencies - spectral_centroid) ** 2) * psd) / np.sum(psd))
    spectral_flatness = np.exp(np.mean(np.log(psd))) / np.mean(psd)
    spectral_kurtosis = kurtosis(psd)
    spectral_skewness = skew(psd)
    roll_off = frequencies[np.where(np.cumsum(psd) >= 0.75 * np.sum(psd))[0][0]]
    peak_psd_freq = frequencies[np.argmax(psd)]
    peak_psd_energy = np.max(psd)
    energy = np.sum(data ** 2)
    crest_factor = np.max(np.abs(data)) / np.sqrt(np.mean(data ** 2))
    zcr = ((data[:-1] * data[1:]) < 0).sum() / len(data)
    skewness_value = skew(data)
    kurtosis_value = kurtosis(data)
    coeffs = pywt.wavedec(data, 'db4', level=3)
    wavelet_entropy = calculate_wavelet_entropy(coeffs[1:])
    low_freq_band = (frequencies >= 0) & (frequencies <= 3)
    high_freq_band = (frequencies > 3) & (frequencies <= 50)
    low_freq_energy = np.sum(psd[low_freq_band])
    high_freq_energy = np.sum(psd[high_freq_band])
    if high_freq_energy == 0:
        high_freq_energy = 1e-10
    freq_band_energy_ratio = low_freq_energy / high_freq_energy
    lower_bound = spectral_centroid - spectral_bandwidth / 2
    upper_bound = spectral_centroid + spectral_bandwidth / 2
    bandwidth_mask = (frequencies >= lower_bound) & (frequencies <= upper_bound)
    bandwidth_energies = psd[bandwidth_mask]
    bandwidth_avg_energy = np.mean(bandwidth_energies)
    return {
        "Spectral Centroid (Hz)": spectral_centroid,
        "Spectral Bandwidth (Hz)": spectral_bandwidth,
        "Spectral Flatness": spectral_flatness,
        "Spectral Kurtosis": spectral_kurtosis,
        "Spectral Skewness": spectral_skewness,
        "Spectral Roll-off Frequency (Hz)": roll_off,
        "Peak PSD Frequency (Hz)": peak_psd_freq,
        "Peak PSD Energy": peak_psd_energy,
        "Energy": energy,
        "Crest Factor": crest_factor,
        "Zero-Crossing Rate": zcr,
        "Skewness": skewness_value,
        "Kurtosis": kurtosis_value,
        "Wavelet Entropy": wavelet_entropy,
        "Frequency Band Energy Ratio": freq_band_energy_ratio,
        "Bandwidth Average Energy": bandwidth_avg_energy
    }

# LSTM Modeli
def define_lstm_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Bidirectional(LSTM(150, activation='selu', return_sequences=True)))
    model.add(Dropout(0.2))
    model.add(Bidirectional(LSTM(100, activation='selu')))
    model.add(Dense(1))
    optimizer = Adamax(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

# CNN Modeli
def define_cnn_model(input_shape):
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))
    model.add(GlobalAveragePooling1D())
    model.add(Dense(50, activation='relu'))
    model.add(Dense(1))
    optimizer = Adamax(learning_rate=0.001)
    model.compile(optimizer=optimizer, loss='mean_squared_error')
    return model

# Ana Fonksiyon
def cross_validation(directory_path):
    fs = 100
    coefficient = 5
    window_size = fs * coefficient
    step_size = window_size // 2
    all_files = sorted([os.path.join(directory_path, f) for f in os.listdir(directory_path) if f.endswith('.csv')])
    save_dir = "prediction_results_compare"
    os.makedirs(save_dir, exist_ok=True)

    for test_file in all_files:
        print(f"Testing with file: {test_file}")
        train_files = [f for f in all_files if f != test_file]
        X_train_list, y_train_list = [], []
        scaler = MinMaxScaler()

        for file_path in train_files:
            data = pd.read_csv(file_path)
            acc_data = data.iloc[:, 0].values
            features_list = [calculate_features(acc_data[i:i+window_size], fs) for i in range(0, len(acc_data) - window_size, step_size)]
            df = pd.DataFrame(features_list)
            scaled = scaler.fit_transform(df)
            lof_scores = calculate_lof(scaled)
            for i in range(20, len(scaled)):
                X_train_list.append(scaled[i-20:i])
                y_train_list.append(lof_scores[i])

        X_train = np.array(X_train_list)
        y_train = np.array(y_train_list).reshape(-1, 1)

        model_lstm = define_lstm_model((20, X_train.shape[2]))
        model_cnn = define_cnn_model((20, X_train.shape[2]))

        model_lstm.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)
        model_cnn.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)

        test_data = pd.read_csv(test_file).iloc[:, 0].values
        test_features = [calculate_features(test_data[i:i+window_size], fs) for i in range(0, len(test_data) - window_size, step_size)]
        test_df = pd.DataFrame(test_features)
        test_scaled = scaler.transform(test_df)
        lof_scores = calculate_lof(test_scaled)

        X_test, y_test = [], []
        for i in range(20, len(test_scaled)):
            X_test.append(test_scaled[i-20:i])
            y_test.append(lof_scores[i])
        X_test = np.array(X_test)
        y_test = np.array(y_test).reshape(-1, 1)

        y_pred_lstm = model_lstm.predict(X_test)
        y_pred_cnn = model_cnn.predict(X_test)

        mse_lstm = mean_squared_error(y_test, y_pred_lstm)
        mse_cnn = mean_squared_error(y_test, y_pred_cnn)

        print(f"{os.path.basename(test_file)} - LSTM MSE: {mse_lstm:.4f} | CNN MSE: {mse_cnn:.4f}")

        plt.figure(figsize=(12, 6))
        plt.plot(y_test, label="Gerçek", color='black')
        plt.plot(y_pred_lstm, label="LSTM Tahmini", linestyle='--', color='blue')
        plt.plot(y_pred_cnn, label="CNN Tahmini", linestyle=':', color='green')
        plt.title(f"LOF Tahmini Karşılaştırması - {os.path.basename(test_file)}")
        plt.xlabel("Pencere Numarası")
        plt.ylabel("LOF Skoru")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, os.path.basename(test_file).replace('.csv', '_compare.png')))
        plt.close()

if __name__ == "__main__":
    cross_validation('veri/TÜM DEPREM/')
